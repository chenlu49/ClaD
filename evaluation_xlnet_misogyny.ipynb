{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ClaD_misogyny import ClaDModel\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "NAME='misogyny'\n",
    "\n",
    "DEV = './simcse-datasets/'+NAME+'/dev.txt'\n",
    "TEST = './simcse-datasets/'+NAME+'/test.txt'\n",
    "\n",
    "model = ClaDModel(pretrained_model='xlnet-base-cased', pooling='cls')\n",
    "state_dict = torch.load('./saved_model_'+NAME+'/simcse_sup_xlnet.pt', map_location=torch.device('cpu'))\n",
    "state_dict.pop(\"bert.embeddings.position_ids\", None)  \n",
    "model.load_state_dict(state_dict) \n",
    "model.eval()\n",
    "# Initialize the tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "sentence = \"This is an example sentence.\"    \n",
    "def convert_to_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs,_ = model(**inputs)\n",
    "        outputs_array=outputs.numpy()[0]\n",
    "        return outputs_array\n",
    "        \n",
    "print(convert_to_embedding(sentence))\n",
    "###train embeddings\n",
    "src_path='./simcse-datasets/'+NAME+'/train.txt'\n",
    "\n",
    "def get_positive_embeddings(path,name):\n",
    "    positive=[]\n",
    "    with jsonlines.open(path, 'r') as reader:\n",
    "        for line in tqdm(reader):\n",
    "            sent1 = line.get(name+'1')\n",
    "            sent2 = line.get(name+'2')\n",
    "            positive.append(sent1)\n",
    "            positive.append(sent2)\n",
    "    positive=list(set(positive))\n",
    "    positive_embeddings=[]\n",
    "    for p in positive:\n",
    "    \n",
    "        positive_embeddings.append(convert_to_embedding(p))\n",
    "\n",
    "    positive_embeddings=np.array(positive_embeddings)\n",
    "    print(positive_embeddings.shape)\n",
    "    return positive_embeddings\n",
    "\n",
    "\n",
    "positive_embeddings=get_positive_embeddings(src_path,NAME)\n",
    "mean=np.mean(positive_embeddings,axis=0)\n",
    "std=np.std(positive_embeddings,axis=0)\n",
    "normalized_data = (positive_embeddings - mean) / std\n",
    "covariance_matrix = np.cov(normalized_data, rowvar=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "test_path='./simcse-datasets/'+NAME+'/test.txt'\n",
    "names=['source','content','label']\n",
    "def load_test_data(path,names):\n",
    "    pred_test=[]\n",
    "    real=[]\n",
    "    origin=[]\n",
    "    comment=[]\n",
    "    with jsonlines.open(path, 'r') as reader:\n",
    "        for line in tqdm(reader):\n",
    "            sent1 = line.get(names[0])\n",
    "            sent2 = line.get(names[1])\n",
    "            label = line.get(names[2])\n",
    "            if sent2 not in comment:\n",
    "                real.append(label)\n",
    "                origin.append(sent1)\n",
    "                comment.append(sent2)\n",
    "    return comment,real\n",
    "comment,real = load_test_data(test_path,names)\n",
    "len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def cal_mah(new_data, data_points):\n",
    "    n = data_points.shape[0]\n",
    "    d = data_points.shape[1]\n",
    "    data = np.vstack((new_data, data_points))\n",
    "    mean = np.mean(data, axis=0)\n",
    "    covariance_matrix = np.cov(data, rowvar=False)\n",
    "\n",
    "    difference_vector = (new_data - mean)\n",
    "    inverse_covariance_matrix = np.linalg.pinv(covariance_matrix)\n",
    "    squared_distance = np.dot(np.dot(difference_vector.T, inverse_covariance_matrix), difference_vector)\n",
    "    return squared_distance\n",
    "\n",
    "def ag1(squared_distance, data_points,alpha):\n",
    "    n = data_points.shape[0]\n",
    "    d = data_points.shape[1]\n",
    "    print(n,d)\n",
    "    T = (n+1)/(n**2) * squared_distance\n",
    "\n",
    "    p = d/2  # Shape parameter alpha of Beta distribution\n",
    "    q = (n-d)/2  # Shape parameter beta of Beta distribution\n",
    "    # Significance level alpha\n",
    "    # Compute critical value for the upper tail (e.g., 95th percentile)\n",
    "    critical_value_upper = stats.beta.ppf(1 - alpha, p, q)\n",
    "    # print(critical_value_upper)\n",
    "    # print(T)\n",
    "    # print('===========')\n",
    "\n",
    "    if T > critical_value_upper:\n",
    "        return 0  # Abnormal (above the critical value)\n",
    "    else:\n",
    "        return 1  # Normal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Squared_distance=[]\n",
    "data = positive_embeddings\n",
    "for i in range(len(comment)):\n",
    "    arr=convert_to_embedding(comment[i])\n",
    "    \n",
    "    s=cal_mah(arr,data)\n",
    "    Squared_distance.append(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "categories = np.array(real)\n",
    "lengths = np.array(Squared_distance)\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['blue' if x == 0 else 'red' for x in categories]  \n",
    "plt.scatter(range(len(lengths)), lengths, c=colors, alpha=0.6)  \n",
    "\n",
    "plt.title('')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Squared Mahalanobis Distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "alpha=[1e-16]\n",
    "results=[]\n",
    "for a in alpha:\n",
    "    pred_using_ag1=[]\n",
    "    for i in range(len(Squared_distance)):\n",
    "        \n",
    "        s=Squared_distance[i]\n",
    "        \n",
    "        result=ag1(s,data,a)\n",
    "        \n",
    "        pred_using_ag1.append(result)\n",
    "    y_true=real\n",
    "    y_pred=pred_using_ag1\n",
    "    # \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    # \n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    # \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    results.append([accuracy,precision,recall,f1])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(f'False Positive Rate: {fpr}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
